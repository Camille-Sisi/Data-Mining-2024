
# (PART\*) Séance 7 : Régressions/Prédictions {-}


# Modèles de régression : application sur le RP 2019 {#c13-Modeles-regression}
<div align="justify">

On cherche maintenant à expliquer un phénomène ou une variable par rapport à d'autres. On va ici s'intéresser au fait d'être en couple plutôt que de ne pas l'être.     
On va repartir de la base RP, en réduisant le champ à Paris et aux 18 ans ou plus.  


## La création des bases d'apprentissage et de test
On crée notre base de travail en ne prenant que les variables qui nous intéressent et le champ le plus pertinent pour cette analyse.  La fonction `relevel()` permet d'indiquer la modalité de référence que l'on veut dans les régressions futures, ici pour la régression logistique.  

```{r}
dt_reg <- RP_final %>% 
  filter(DEPT=="75" & !AGER20 %in% c("2","5", "10", "14","17")) %>% 
  mutate(age = as.factor(case_when(AGER20 %in% c("19", "24") ~ "18-24ans",
                                   AGER20 %in% c("29","39", "54") ~ "25-54ans",
                                   AGER20 %in% c("64","79", "80") ~ "55ansouplus")),
         couple = as.factor(case_when(COUPLE == "1" ~ "oui", TRUE ~ "non")),
         couple = relevel(couple, ref="non"),
         immigres = as.factor(case_when(IMMI=="1" ~"oui", TRUE ~ "non")),
         sexe = as.factor(case_when(SEXE == "1" ~ "homme", TRUE ~ "femme")),
         sexe = relevel(sexe, ref="homme"),
         emploi = as.factor(case_when(TACT == "11" ~ "en_emploi",
                                      TRUE ~ "non_emploi")),
         emploi = relevel(emploi, ref="non_emploi"),
         chômage = as.factor(case_when(TACT == "12" ~ "au_chômage", 
                                       TRUE ~ "non_chômage")),
         chômage = relevel(chômage, ref="non_chômage"),
         diplome= as.factor(case_when(DIPL %in% c("01", "02", "03", "11", "12", 
                                                  "13") ~ "Peu_pas_dipl",
                                      DIPL %in% c("14", "15") ~ "Bac",
                                      DIPL %in% c("16", "17") ~ "Bac+2-3-4",
                                      DIPL %in% c("18", "19") ~ "Bac+5ou+")),
         diplome = relevel(diplome, ref="Peu_pas_dipl")) %>% 
  select(age, couple, immigres, sexe, emploi, chômage, diplome) 

# On peut éventuellement étudier en statistiques descriptives les liens entre la
# variable d'intérêt "couple" et les autres variables avec le code suivant :
# dt_reg %>% group_by(couple) %>% 
#   count(diplome) %>% pivot_wider(names_from = diplome, values_from = "n",
#                                  names_prefix = "diplome_") %>%
#   adorn_totals(c("row",'col')) %>% adorn_percentages("col") %>%
#   adorn_pct_formatting(digits=2)

```


De manière traditionnelle, dans les modèles de prédiction ou *machine learning*, on n'applique pas le modèle sur l'ensemble de la base de données mais d'abord sur un échantillon dit d'apprentissage puis on le "teste" sur l'échantillon restant. On va donc ici suivre ce schéma et diviser notre base de données en deux pour avoir un échantillon d'apprentissage ou d'entraînement, et un autre test.  
On utilise pour cela la fonction `sample` (mais d'autres fonctions existent) en lui spécifiant la façon de diviser la base avec l'argument `prob=` : ici on choisit de diviser notre base selon un rapport 70% *vs* 30%, autrement dit notre base d'apprentissage comprendra 70% des données de la base initiale, alors que la base de test comprendra les 30% restants. On pourrait procéder à un rapport du type 80% *vs* 20%, ou 75% *vs* 25%, etc.
```{r}
# On choisit la façon de diviser notre base et on l'applique en créant 2 bases
sample <- sample(c(TRUE, FALSE), nrow(dt_reg), replace=TRUE, prob=c(0.70,0.3))
dt_reg_train <- dt_reg[sample, ]
dt_reg_test  <- dt_reg[!sample, ]

# On regarde quelle est la taille de nos deux bases
dim(dt_reg_train)
dim(dt_reg_test)

# On vérifie que les proportions de notre variable d'intérêt sont assez proches
# entre les deux bases
dt_reg_train %>% tabyl(couple) %>% adorn_pct_formatting() %>% 
  adorn_totals("row") %>% gt()
dt_reg_test %>% tabyl(couple) %>% adorn_pct_formatting() %>% 
  adorn_totals("row") %>% gt()
```

Les deux bases présentent une répartition couple/non couple très proche : à Paris, environ 48% de la population est en couple.  

 

## Un modèle à visée principale explicative : la régression logistique
La fonction `glm` du package **`stats`** (à installer avant appel dans la librarie) est principalement utilisée pour modéliser différents types de régression : l'argument `family=binomial("logit")` permet d'utiliser un modèle logit.    
```{r}
# install.packages("stats")
library(stats)
```

### Le modèle initial
On crée le modèle en spécifiant la variable d'intérêt puis les variables explicatives ou l'ensemble des variables présentes dans la base si nous avons déjà procédé à une sélection des variables : c'est le cas ici donc c'est pour cela que l'on indique juste un "." après le "~", sinon on devrait écrire les variables une par une, ou les sctoker dans une liste et appeler la liste.    
 
Même si ce type modèle doit permettre essentiellement d'expliquer un phénomène, ici être en couple par rapport à ne pas l'être, on peut l'utiliser aussi pour prédire les données. C'est pourquoi nous appliquons d'abord le logit sur la base (réduite) d'apprentissage.
```{r}
logit_1 <- glm (couple ~ ., data=dt_reg_train, family = binomial("logit"))
summary(logit_1)
```
Toutes les variables sont significatives.

<!--On peut ensuite évaluer la significativité globale du modèle, : -->
```{r eval=FALSE, include=FALSE}
chi2 <- logit_1$null.deviance - logit_1$deviance
ddl <- logit_1$df.null - logit_1$df.residual
pvalue <- pchisq(chi2, ddl, lower.tail = F)
chi2
ddl
pvalue
```
  
   
On va donc l'appliquer maintenant à la base test, en créant des indicateurs mesurant le taux de prédiction ou au contraire d'erreur.  

Le modèle `predict` permet d'abord d'abord de calculer la probabilité d'être en couple pour chaque individu, l'argument `type="response"` permettant d'appliquer le modèle logistique. Il est plus intéressant d'avoir la probabilité d'une variable de type qualitative, "oui"/"non" comme la variable d'intérêt du modèle, il faut donc procéder à une transformation aboutissant à une nouvelle variable (on considère alors que si la probabilité est strictement supérieure à 0.50 alors cela équivaut à une modalité "oui"). Enfin, on crée une matrice de confusion qui est en réalité un tableau croisé entre les valeurs observées et les prédicitions du modèle ; et on calcule un taux d'erreur en rapportant la somme des éléments hors diagonale principale à la somme des observations totales (de la matrice donc).
```{r}
# Modèle de prédiction pour récupérer les probabilités individuelles d'être en couple
pred.proba <- predict(logit_1, newdata = dt_reg_test, type="response")

# On transforme les probas en variable qualitative
pred.moda <- factor(case_when(pred.proba>0.5 ~ "oui", TRUE ~ "non"))

# On crée la matrice de confusion
matrice_conf <- table(dt_reg_test$couple, pred.moda)
matrice_conf

# On calcule le taux d'erreur
tx_erreur <- (matrice_conf[2,1]+matrice_conf[1,2])/sum(matrice_conf)
tx_erreur * 100
```
On voit que parmi la modalité observée "non" de la base de données, le modèle prédit 59 825 non soit 55% de cette modalité, c'est la majorité mais cela ne semble pas non plus très élevé ; la prédiction est meilleure si on regarde la modalité "oui", puisque 71 564 observations, soit 69% de cette modalité, se retrouvent bien en "oui". Le taux d'erreur de 38% montre bien que le modèle ne prédit pas hyper bien.   
  
  
Une autre façon de faire est après les deux premières étapes réécrites ici, d'ajouter la variable de prédiction à la table initiale est de créer une nouvelle variable qui indique si on a bien une correspondance entre les modalités des deux variables : l'initiale et celle prédite. Cela nous donnc donc le taux d'erreur calculé au-dessus, on retrouve bien 38,4% de correspondances inexactes. 
```{r}
# Modèle de prédiction pour récupérer les probabilités individuelles d'être en couple
#pred.proba <- predict(logit_1, newdata = dt_reg_test, type="response")

# On transforme les probas en variable qualitative
#pred.moda <- factor(case_when(pred.proba>0.5 ~ "oui", TRUE ~ "non"))

# On ajoute la variable de prédiction dans la table test initiale
dt_reg_test_bis <- cbind.data.frame(dt_reg_test, var_predict=pred.moda)
# et on créer une variable indiquant ou non la correspondance entre
# les modalités des deux variables
dt_reg_test_bis %>% 
  mutate(predict_OK=as.factor(case_when(couple=="oui" & var_predict=="oui" ~ "oui", 
                                        couple=="non" & var_predict=="non" ~ "oui",
                                        TRUE ~ "non"))) %>% 
  tabyl(predict_OK) %>% adorn_pct_formatting() %>% adorn_totals("row")
```


  
  
### L'évaluation du modèle et la recherche éventuelle d'un "meilleur" modèle
On peut améliorer le modèle en recherchant celui qui est le "meilleur" en faisant une sélection sur les variables, plus précisément en demandant au modèle de choisir les variables les plus explicatives, car peut-être que certaines ne sont pas nécessaires à l'explication du modèle (dans notre cas, nous avons néanmoins vu que toutes les variables étaient significatives donc a priori utiles dans le modèle).   

Pour une sélection "pas à pas", il faut utiliser le package **`MASS`** et la fonction `stepAIC` car c'est à travers le critère AIC ("Akaike Information Criterion",) que le modèle va chercher à être "meilleur" :  plus il sera faible, meilleur il sera. On va d'abord faire cette sélection de façon "descendante" c'est-à-dire en partant du modèle initial "logit_1" ici : on part du modèle avec l'ensemble des variables et on en supprime une au fur et à mesure pour voir si le modèle est "meilleur".
```{r warning=FALSE, message=FALSE}
# install.packages("MASS)
library(MASS)
logit_backward <- stepAIC(logit_1, 
                          scope=list(lower="couple ~ 1", 
                          upper="couple ~ age +  immigres + sexe + emploi + chômage + 
                          diplome"),
                          direction="backward")
logit_backward
```

La sortie n'affiche que le "meilleur" modèle, et c'est donc bien celui qu'on avait mis avec l'ensemble des variables.  

  
Pour une sélection des variables de façon "ascendante", en partant d'un modèle sans variable puis on ajoute une à une les variables. On utilise la même fonction mais en changeant les paramètres et en créant un modèle "vide" avant : 
```{r}
logit_0 <- glm(couple ~ 1, data=dt_reg_train, family=binomial("logit"))
logit_forward <- stepAIC(logit_0, 
                         scope=list(lower="couple ~ 1", 
                         upper="couple ~ age +  immigres + sexe + emploi + chômage + diplome"),
                         direction="forward")
logit_forward
```

Le taux d'AIC diminue à chaque variable ajoutée donc le meilleur modèle est bien celui avec l'ensemble des variables explicatives mises dans le modèle initial.  

```{r eval=FALSE, include=FALSE}
# Autre façon de tester le modèle avec la fonction drop1:
drop1(logit_1, test="Chisq")
```



### Le modèle final et l'interprétation des résultats
Finalement, dans une visée plus explicative que prédictive, on peut estimer notre modèle sur l'ensemble de la base et étudier plus précisément les résultats avec les *odds-ratios* ou rapports des côtes par exemple pour commenter plus directement les coefficients. Ces *odds-ratios* correspondent à l’exponentiel des coefficients initiaux de la régression. Ils se lisent par rapport à 1 : il est égale à 1 si les deux côtes sont identiques donc s'il n'y a pas de différence de probabilité d'être en couple selon qu'on est une femme ou un homme, il est supérieur à 1 si ici la femme a une probabilité supérieure à celle de l'homme d'être en couple, et il est inférieur à 1 si la femme a une probabilité inférieure à celle de l'homme d'être en couple.   
```{r warning=FALSE, message=FALSE}
logit_VF <- glm (couple ~ ., data=dt_reg, family = binomial("logit"))
summary(logit_VF)

# Résumé des résultats sous forme de tableau avec les odds-ratio
# avec la librairie "questionr" d'abord
#install.packages(questionr)
library(questionr)
odds.ratio(logit_VF)

# puis avec la librarie "forestmodel" pour avoir un meilleur rendu
#install.packages("forestmodel")
library(forestmodel)
forest_model(logit_VF)

# ou encore avec "gt_summary" pour avoir un autre rendu
library(gtsummary)
theme_gtsummary_language("fr", decimal.mark = ",", big.mark=" ")
logit_VF %>% 
  tbl_regression(exponentiate = TRUE) %>% 
  add_global_p(keep=TRUE) %>% 
  modify_header(label ~ "**Variable**") %>% 
  modify_caption("**Tableau de résultats. Variables expliquant le fait d'être en couple à Paris**")
```
  
On privilégiera plutôt les deux derniers types de tableaux : ainsi, on note que la probabilité d'être en couple est 7,86 fois plus élevée si on a entre 25 et 54 ans et 9,09 fois plus élevée si on a 55 ans ou plus ; c'est donc l'âge le principal déterminant dans ce modèle du fait d'être en couple. Les autres variables sont néanmoins aussi significatives mais d'un ordre de grandeur moindre : la probabilité d'être en couple est 1,39 fois plus élevée si on est immigré, elle est 1,37 fois plus élevée si on est en emploi, ou encore elle est plus élevée plus on est diplômé au-delà du bac Dit autrement, la probabilité d'être en couple augmente de 39% ((1.39-1)*100) si on est immigré par rapport au fait de ne pas l'être. En revanche, la probabilité d'être en couple est plus faible si on est au chômage de 0.81 fois, autrement dit elle diminue de 19% (1-0.81), elle l'est également si on est une femme (0.69, soit -31%), ou si on a le bac par rapport à ne pas avoir de diplôme (0.97, soit -3%).  

On peut aussi vouloir visualiser ces résultats, on peut pour cela utiliser la librarie **`GGally`** et la fonction `ggcoef_model()`.
``` {R message=FALSE}
# Résumé des résultats sous forme graphqiue
library(GGally)
ggcoef_model(logit_VF, exponentiate = TRUE) +
  ggtitle("Varibles expliquant le fait d'être en couple à Paris") 
```
C'est peut-être le meilleur rendu...
  

  


## Un modèle à visée principale prédictive : l'abre de décision
On va procéder à la même analyse en cherchant, cette fois, à prédire si un individu sera en couple ou non à partir des variables sélectionnées précédemment.  
On repart donc des deux tables créées et on va utiliser un modèle dit d'apprentissage supervisé, l'abre de décision. Sa construction repose sur un partitionnement récursif des observations qui se fait à partir de noeuds coupés, ces coupures pourront répondre à des règles et des conditions à spécifier ou faire varier pour avoir un meilleur modèle.  
Ici on va utiliser un arbre de classification puisque notre variable est qualitative (binaire).  
  
C'est le package **`rpart`** qui est spécialisé dans les modèles d'arbres de décision, à installer donc d'abord puis à appeler dans la librairie ; le package **`rpart.plot`** permet, lui, d'avoir un arbre plus esthétique et informatif.    
```{r warning=FALSE, message=FALSE}
# install.packages("rpart")
# install.packages("rpart.plot")
library(rpart)
library(rpart.plot)
```
  

### Le modèle initial
La spécification du modèle est assez simple, on précise la variable d'intérêt, les éventuelles variables explicatives ou toutes celles qui sont dans la table avec le `.` (comme précédemment pour le modèle logit), la base de données sur laquelle appliquer le modèle, et dans l'argument `method=` on spécifie le type de modèle, soit "class" pour une variable d'intérêt qualitative ou binaire, soit "anova" pour une variable d'intérêt quantitative ou continue.  

On va d'abord appliquer le modèle sur notre échantillon d'apprentissage ou d'entraînement : 
```{r}
arbre_1 <- rpart(couple ~ ., data=dt_reg_train, method="class")
arbre_1
```
Le modèle nous donne d'abord les résultats en format texte, ce sont des indications sur les différentes "noeuds" puis "branches" de l'arbre, etc. : "node" pour noeud et son numéro, "split" pour la condition de coupure/le critère de décision, "n" pour le nombre total d'observations dans un noeud, "loss" le nombre d'observations qui n'appartient pas à la modalité prédite, "yval" la modalité prédite pour les individus présents à l'étape du noeud, et "yprob" la proportion d'observations pour les individus présents à l'étape du noeud qui prend la valeur prédite en seconde position. Le petit astérix "*" précise que le noeud est une feuille ("terminal").   

Par exemple, ici le premier noeud indiqué "root" représente l'ensemble de l'échantillon (c'est la "racine" de l'arbre), soit 497 765 observations, et comme la modalité prédite est "non", il y a 51,7% d'observations qui ne sont pas en couple, contre 48% qui le sont soit 240 358 observations (celles qu'on perd à cette étape donc). La première variable discriminante est l'âge : ceux ayant entre 18 et 24 ans forment une première branche et un groupe qui n'est pas en couple, alors que les plus de 25 ans forment l'autre branche et un groupe qui est en couple. Ensuite, pour ce dernier groupe, une autre division se forme selon le sexe : être une femme va former un autre groupe qui ne sera pas en couple, lui-même se divisera encore en deux groupes selon l'âge avec la tranche des 55 ans ou plus qui ne seront pas en couple. Etc.   

On peut regarder quelles sont les variables les plus importantes dans le modèle par ordre : 
```{r}
arbre_1$variable.importance
```
Le diplôme est la variable la plus discriminante, comme on l'avait supposé puisque c'était la première variable qui divisait notre échantillon, ensuite vient le fait d'être une femme ou un homme, etc., et en tout dernier le diplôme.  


On va mieux étudier cela avec le résultat visuel.  
Pour avoir ainsi graphiquement l'arbre, il faut appeler la fonction `rpart.plot()` du même package, l'argument "extra" permettant de préciser le type de modèle : "106" pour des modèles en classes avec une variable qualitative et binaire, "104" pour des modèles en classes mais avec une variable d'intérêt qualitative avec plus de 2 modalités, et "100" pour les autres modèles.
```{r}
# On dessine l'arbre
rpart.plot(arbre_1, extra=106)

# ou avec la librarie `rattle`
# install.packages("rattle")
# library(rattle)
# fancyRpartPlot(arbre_1)
```
Au sommet de l'arbre on a donc la racine (qui est le 1er noeud), puis il se divise en 2 branches pour aboutir à deux autres noeuds, etc.  
On voit donc que la branche partant sur la gauche, c'est le cas où la variable d'âge est égale à la modalité "18-24 ans" car on voit le "yes" qui est encadré (et ça sera le cas à chaque fois même si ce n'est pas de nouveau inscrit, autrement dit la branche partant sur la gauche sera la modalité "oui" de la variable).    
Il y a chaque fois 3 données indiquées : 

- d'abord, la modalité prédite par le modèle lorsqu'on est dans le groupe considéré, par exemple pour la feuille terminale à gauche, donc pour les 18-24 ans, la modalité prédite sera le "non" donc pas en couple, on pourra vérifier dans les données en récupérant la modalité prédite pour chaque observation, pour tous les individus de 18-24 ans la modalité prédite est "non" ; 
- ensuite, parmi les individus de ce groupe, donc les 18-24 ans, 11% sont en couple, ; 
- et enfin, les 18-24 ans représentent 13% de la population (de notre base d'apprentissage).  
Autrement dit, 13% des individus de notre population ont 18-24 ans avec une probabilité d'être en couple de 13%.  
  
L'autre branche indique les individidus qui prennent toutes les autres modalités de la variable, soit les 25 ans ou plus, pour lesquels la modalité prédite sera oui, ils ont 54% à être en couple et la probabilité prédite sera de 87%.  
Tout en bas, se trouvent les feuilles de l'arbre, c'est lorsqu'il n'y a plus aucune branche qui part du noeud en question. 

Ainsi, 3% de la base (d'apprentissage) sont des individus de plus de 24 ans, hômmes et au chômage, qui ont une probabilité de 42% d'être en couple ; ou encore, 19% de notre population (base d'apprentissage) sont des femmes de 55 ans ou plus et qui ont une probabilité de 42% également d'être en couple.  

 

### L'évaluation du modèle
De la même façon que précédemment, on peut vérifier la bonne (ou non) prédiction du modèle en l'appliquant sur l'échantillon dit test, puis en comparant les proportions prédites avec celles effectivement observées dans la base dans la matrice de confusion et enfin en calculant un taux de concordance ou au contraire un taux d'erreur à partir de cette une matrice de confusion : 
```{r}
# Modèle appliqué sur l'échantillon test
predict_test <- predict(arbre_1, dt_reg_test, type="class")

# Comparaison des résultats - Matrice de confusion
mat_confusion <- table(dt_reg_test$couple, predict_test)
mat_confusion

# Taux de concordance : rapport entre la somme des éléments 
# de la diagonale principale et la somme des observations 
# totales (soit de la matrice)
tx_concordance <- sum(diag(mat_confusion) / sum(mat_confusion))
tx_concordance * 100
# Taux d'erreur
tx_erreur <- (mat_confusion[2,1] + mat_confusion[1, 2]) / sum(mat_confusion)
tx_erreur * 100
```
  
  
On peut regarder aussi ce que cela donne sur la base d'apprentissage.
```{r}
predict_train <- predict(arbre_1, dt_reg_train, type="class")
mat_confusion_1 <- table(dt_reg_train$couple, predict_train)
mat_confusion_1
tx_erreur_1 <- (mat_confusion_1[2,1] + mat_confusion_1[1, 2]) / sum(mat_confusion_1)
tx_erreur_1 * 100
```

Dans les deux cas, les taux d'erreur sont assez élevés, d'environ 38%. Le modèle ne prédit pas hyper bien. On peut noter qu'on a pratiquement le même taux d'erreur que le modèle logit réalisé précédemment...  

  


Vérifions si nous pouvons l'améliorer en modifiant les paramètres de construction de l'arbre, c'est-à-dire en jouant sur les conditions de coupure d'un noeud et sur les règles d'arrêt de ces coupures. On l'effectue avec la fonction `rpart.control()` avec les arguments suivants (règles d'arrêt principalement) : `minsplit=` donne le nombre minimum d'observations (individus) présentes à l'étape d'un noeud pour envisager une coupure ; `minbucket=` qui donne le nombre minimum d'observations/individus présentes à l'étape d'un noeud qu'engendrerait la coupure du noeud parent ; `maxdepth` qui donne la profondeur de l'arbre ; et `cp=` qui est un paramètre de complexité (plus il est petit, plus grand est l'arbre de régression).  

```{r}
# Définition des règles de décision
ajust_param <- rpart.control(minsplit = 50, minbucket = 50, maxdepth = 10, cp=0)

# Ajustement du modèle en indiquant le paramètre "control"
arbre_2 <- rpart(couple ~ ., data=dt_reg_train, method="class", 
                 control = ajust_param)

# On étudie de nouveau la matrice de confusion et 
# le taux d'erreur associé au nouveau modèle sur la base test
predict_test_1 <- predict(arbre_2, dt_reg_test, type="class")
mat_confusion_ajust <- table(dt_reg_test$couple, predict_test_1)
mat_confusion_ajust
tx_erreur_1 <- (mat_confusion_ajust[2,1] + mat_confusion_ajust[1, 2]) / sum(mat_confusion_ajust) *100
tx_erreur_1
```

On peut l'appliquer de même à l'échantillon d'apprentissage :
```{r}
# Modèle appliqué sur l'échantillon test
predict_train <- predict(arbre_2, dt_reg_train, type="class")

# Comparaison des résultats - Matrice de confusion
mat_confusion_ajust1 <- table(dt_reg_train$couple, predict_train)
mat_confusion_ajust1

# Taux d'erreur
tx_erreur_2 <- (mat_confusion_ajust1[2,1] + mat_confusion_ajust1[1, 2]) / sum(mat_confusion_ajust1)* 100
tx_erreur_2 
```
Le taux d'erreur est un peu plus faible, la différence est légère, mais c'est toujours ça de pris !   
  
  
L'arbre correspondant est le suivant: 
```{r}
rpart.plot(arbre_2, extra=106)
```
On voit qu'il est bien moins lisible car plus profond, mais il prédit mieux (mais peut-être trop bien - cf. risque de surapprentissage en *matching learning*).  


<!--Enfin, on peut chercher à minimiser l'erreur de prédiction de l'arbre afin de définir le niveau d'élagage optimal permettant ensuite de simplifier l'arbre.
``` {R}
# 2 fonctions liées à l'argument "cp=" de notre modèle
printcp(arbre_1)
plotcp(arbre_1)

```
Ici il faut donc indiquer la valeur 0.010 dans l'argument `cp=` pour minimiser l'erreur relative, c'est notre nouvelle règle d'arrêt. On va par conséquent reconstruire l'abre à partir de la fonction `prune()` en indiquant cette valeur optimale pour l'élaguer :  
```{r}
min(arbre_1$cptable[, "xerror"]) + (1*arbre_1$cptable[ which.min(arbre_1$cptable[, "xerror"]), "xstd"])
opt <- which.min(arbre_1$cptable[, "xerror"])
opt
cp <- arbre_1$cptable[opt, "CP"]

arbre_VF <- prune(arbre_1, cp=cp)
rpart.plot(arbre_VF)
```
Cela ne change en réalité rien ici ! --> 

  
Vous pouvez réitérer l'exercice en introduisant d'autres variables dans l'analyse, en changeant la variable cible, etc.  


Pour aller plus loin, on utilise maintenant beaucoup en analyse prédictive les forêts d'arbre de décision, qui constituent comme son nom l'indique, un ensemble d'arbres de régression, ce qui permet d'avoir un taux d'erreur global moindre, car la principale critique de l'arbre de décision est son potentiel d'erreur.

