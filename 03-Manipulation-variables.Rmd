
# Manipulation des variables {#c03-Manipulation-variables}

<div align="justify">

Dans la manipulation des variables, l'une des premières choses à réaliser est de les définir dans le bon format, variables quantitatives/continues ou variables qualitatives/catégorielles.

On l'a vu dans la section précédente, certaines variables sont encore codées comme des nombres entiers ("integer") alors que ce sont des variables catégorielles. On va donc corriger cela en regardant d'abord quelles variables sont concernées, en les sélectionnant avec `select_if()` ou `select(where())` :

```{r}
RP_final %>% 
  select_if(is.numeric) %>%
  names()
```

A part les variables d'âge `AGED` et `AGEREV`, de date de naissance `ANAI` et de pondération `IPONDI`, toutes les autres variables devraient en format "factor".

```{r}
RP_final <- RP_final %>% mutate(AGER20=as.factor(AGER20),
                                AGEREVQ=as.factor(AGEREVQ),
                                CATPC=as.factor(CATPC),
                                COUPLE=as.factor(COUPLE),
                                CS1=as.factor(CS1),
                                DEPT=as.factor(DEPT),
                                ETUD=as.factor(ETUD),
                                IMMI=as.factor(IMMI),
                                INAI=as.factor(INAI),
                                INATC=as.factor(INATC),
                                MOCO=as.factor(MOCO),
                                MODV=as.factor(MODV),
                                NAIDT=as.factor(NAIDT),
                                ORIDT=as.factor(ORIDT),
                                REGION=as.factor(REGION),
                                SEXE=as.factor(SEXE),
                                STAT_CONJ=as.factor(STAT_CONJ),
                                TACT=as.factor(TACT),
                                TACTD16=as.factor(TACTD16))
```

On peut ensuite vérifier que ces variables sont bien des variables facteurs en regardant combien de modalités elles ont et quelles sont-elles. Par exemple, pour la variable `CATPC` :

```{r}
nlevels(RP_final$CATPC)
levels(RP_final$CATPC)
```

Si nous n'avions pas mis l'option transformant les variables caractères en variables facteurs lors du chargement des données, nous pourrions le faire maintenant en utilisant la fonction `mutate_if` ou la combinaison de `mutate` et `across(where())` comme ceci `RP %>% mutate_if(is.character, as.factor)` ou `RP %>% mutate(across(where(is.character), as.factor))`.

On peut enfin vérifier quelles sont les variables numériques qui restent :

```{r}
# RP_final %>% select_if(is.numeric) %>% head() %>% gt()
RP_final %>% select(where(is.numeric)) %>% head() %>% gt()

```

Plus généralement, il est souvent d'usage d'utiliser la fonction `summary()` pour donner un aperçu de l'ensemble des variables, soit de leur distribution pour les variables quantitatives, soit de leur répartition par modalités pour les variables qualitatives ; la fonction permet également de nous donner l'information sur l'existence et le nombre de valeurs manquantes pour chaque variable.

```{r}
summary(RP_final)
```

Mais attention, le problème ici est que cela nous donne des fréquences non pondérées pour l'ensemble de nos variables qualitatives, donc qui n'ont finalement pas grand sens.

## Manipulation des variables qualitatives

On peut d'abord travailler sur les variables qualitatives qui correspondent ici à l'essentiel de nos variables.

Comme on le sait, on peut regarder les différents niveaux pour chacune d'entre elles, avec la fonction `levels()`. Si on veut appliquer la fonction à l'ensemble de nos variables facteurs sans avoir donc à les indiquer une par une, on peut avoir recours à la fonction `sapply()` qui permet d'appliquer la fonction indiquée entre parenthèses (ici `levels()`) à tous les éléments de notre table de données.

```{r eval=FALSE}
# Pour info, ici cela s'écrirait : 
RP_final %>% select(where(is.factor)) %>% sapply(levels)
# on peut même se passer de la sélection sur les variables :
# RP %>% sapply(levels)
```

On peut ensuite vouloir retravailler les modalités de ces variables, car par exemple les modalités ne sont pas parlantes puisque nommées par des codes chiffres, ou parce que les modalités sont trop nombreuses et qu'on souhaiterait les rassembler pour une analyse ultérieure.

Par exemple, si l'on veut étudier la répartition de la population francilienne selon leur statut d'activité, on peut utiliser la variable `TACT`:

```{r}
levels(RP_final$TACT)
```

Mais le moins qu'on puisse dire c'est que les 7 modalités de cette variable ne sont pas parlantes, on peut donc recoder les modalités de cette variable dans une étape préalable *DATA* comme ici ; on pourra bien sûr enchaîner plus tard les lignes de codes et réaliser cette étape dans une même procédure avec le tableau ou le graphique représentant cette variable.\
Commençons ici par l'étape *DATA* :

```{r}
# On cherche à quoi correspondent les modalités chiffrées de cette variable
# dans le fichier "meta"
meta %>% filter(COD_VAR=="TACT") %>% select(COD_MOD, LIB_MOD)
```

```{r}
# On recode à partir de ces libellés, tout en regroupant certaines modalités
# qui sont très spécifiques et nous intéressent moins :
RP_final <- RP_final %>% mutate(TACT_moda = as.factor(
  case_when(TACT == "11" ~ "Actifs en emploi",
            TACT == "12" ~ "Chômeurs",
            TACT == "21" ~ "Retraités",
            TACT %in% c("22","23","24","25") ~ "Autres inactifs")))
levels(RP_final$TACT_moda)
```

Si l'on veut changer l'ordre des modalités, qui s'afficheront comme ci-dessus dans un tableau ou un graphique, on peut utiliser la fonction `fct_relevel()` du package **`forcats`** (à installer avant puis à appeler avant de l'utiliser) :

```{r}
# install.package("forcats")
library(forcats)
RP_final <- RP_final %>% mutate(TACT_moda = fct_relevel(TACT_moda, 
                                                        c("Actifs en emploi","Chômeurs","Retraités", "Autres inactifs")))
levels(RP_final$TACT_moda)
```

Plus largement, pour travailler sur des variables qualitatives en particulier lorsqu'elles sont en format facteur, le package **`forcats`** est très utile. Outre une fonction de transformation d'une variable caractère en facteur (`as_factor()` proche de la version de baseR `as.factor()` utilisée en début de section), elle contient plein d'autres fonctions : `fct_collapse()` utilisée pour renommer ou regrouper des modalités d'une variable (au lieu de la double fonction `as.factor()` et `case_when()`) ; `fct_relevel()` utilisée également au-dessus pour trier les modalités comme on le souhaite ; `fct_drop()` pour enlever des niveaux de facteurs vides/sans effectifs ; `fct_explicit_na()` pour rendre les NA explicites en créant une modalité "(missing)" ; `fct_reorder()` et `fct_reorder2()` pour réordonner les modalités d'une variable, très utile pour les graphiques car utilisables directement dans `ggplot()` ; `fct_lump()` pour regrouper les modalités les plus communes (ou au contraire les moins communes) en lui indiquant entre parenthèses le nombre `n=` de modalités souhaitées ou la proportion minimum souhaitée `prop=`, et en sélectionnant la variable avec la fonction `pull()` avant car elle doit être en format vecteur et non data.frame ; ou encore `fct_recode()` pour changer le niveau des facteurs ; `fct_other()` ; `fct_infreq()` et `fct_inorder()` ; etc. Un bon récapitulatif de ces fonctions est présenté <a href="https://thinkr.fr/forcats-forcats-vous-avez-dit-forcats/" target="_blank">ici</a>.

## Manipulation des variables quantitatives

Comme nous l'avons vu plus haut, il y a peu de variables quantitatives dans cette base et l'une d'entre elles est la pondération, donc on va regarder plus précisément la variable `AGED`. Cependant, celle-ci aussi est particulière car c'est une variable numérique constituée d'entiers naturels (et non de valeurs réelles) qui vont de 0 à 120 ; dans le fichier des métadonnées (ou le dictionnaire des variables disponible également sur le site de l'Insee), on se rend compte que la variable a été pensée comme catégorielle avec des modalités d'abord codées comme "000", "001", etc.

```{r}
meta %>% filter(COD_VAR=="AGED") %>% select(COD_MOD, LIB_MOD) %>% tail()
```

On peut alors regarder rapidement la distribution de cette variable.

```{r}
summary(RP_final$AGED)
```

On peut aussi construire des variables continues en agrégeant certaines informations au niveau des communes par exemple. Reprenons la variable d'activité dont nous avons recoder et regrouper les modalités et calculons-là pour avoir le nombre de chaque modalité par commune. Il faut pour cela créer la variable de commune, qu'on appelera `COM`, à partir de l'IRIS :

```{r}
RP_final <- RP_final %>% mutate(COM=substr(IRIS, 1, 5))
```

On va ensuite sommer chaque modalité de la variable `TACT_moda` en utilisant la pondération en groupant par commune.

**EXERCICE** :\
Créer donc un tableau qui aura 3 colonnes `COM`, `TACT_moda` et `n`. Vous pouvez utiliser les fonctions `group_by` suivi soit de `count`, soit de `summarise` ; on cherchera finalement à arrondir ces valeurs à l'unité avec la fonction `round()`. Vous devez obtenir le tableau suivant :

```{r echo=FALSE, include=FALSE}
Tab_com_TACT <- RP_final %>% group_by(COM) %>%
  count(TACT_moda, wt=IPONDI) %>% 
  mutate(n=round(n))

Tab_com_TACT
```

::: solution-exo
```{r eval = FALSE}
Tab_com_TACT <- RP_final %>% group_by(COM) %>%
  count(TACT_moda, wt=IPONDI) %>% 
  mutate(n=round(n))

# RP_final %>% group_by(COM, TACT_moda) %>% 
#   summarise(n=sum(IPONDI)) %>% 
#   mutate(n=round(n))
```
:::

\fi

On voit qu'on a un tableau dans un format "long" puisqu'il y a plusieurs observations pour une seule commune. On va utiliser la fonction `pivot_wider()` mentionnée précédemment pour n'avoir qu'une ligne par commune et en colonne les types de statut avec leur nombre respectif.

```{r}
Tab_com_TACT <- Tab_com_TACT %>% 
                   pivot_wider(names_from = TACT_moda, values_from = n)
Tab_com_TACT
```
   
  

### Détecter et "visualiser" les valeurs manquantes

Pour travailler sur les valeurs manquantes et valeurs aberrantes de variables quantitatives, on va s'appuyer sur une autre base de données, plus pertinente pour cela. Vous la trouverez sur l'espace de cours sur Moodle : il s'agit d'une extraction de la base des données de valeurs foncières pour le seul département de Paris et la vente d'appartements entre 2019 et 2022.

Une fois copiée dans le dossier "data/" de notre projet, ouvrons cette base de données et commençons l'exploration des variables de cette base et de leurs valeurs manquantes :

```{r warning=FALSE, message=FALSE}
dvf_Paris <- readRDS(file ="data/dvf_Paris.Rdata")
dvf_Paris %>% head() %>% gt()
summary(dvf_Paris)
```

La fonction `summary()` permet en effet de donner une première information sur les valeurs manquantes des différentes variables. Pour se concentrer sur cette seule information, on peut compter le nombre de valeurs manquantes `NA` pour chacune des variables avec la fonction `colSums()` ; pour les avoir en proportion du nombre total d'observations (lignes), on peut utiliser la fonction `colMeans()` ; sinon ; on peut utiliser la fonction `summarise` combinée avec `across(where()), )` :

```{r}
colSums(is.na(dvf_Paris))

# Pour les avoir en proportion par rapport au nombre total d'observations
# et arrondies à 2 chiffres après la virgule :
round(colMeans(is.na(dvf_Paris)*100), 2)

# Ou en langage tidyverse sur les seules variables numériques :
dvf_Paris %>% summarise(across(where(is.numeric), ~ sum(is.na(.))))
```

Le moins qu'on puisse dire c'est qu'il y a des valeurs manquantes, mais dont le nombre et la proportion (par rapport au nombre total d'observations) varie énormément.

Pour en faire une analyse plus poussée, différents packages existent pour détecter et visualiser ces données manquantes. L'un d'entre eux est le package **`naniar`** : quelques fonctions permettent d'abord de décrire la base selon ses valeurs manquantes. Cela donne un aperçu global et rapide, mais cela n'est vraiment pas suffisant pour comprendre l'origine et les enjeux (possibles problèmes) de ces valeurs manquantes.

```{r}
library(naniar)
# Ci-dessous : nombre de cellules du tableau ou de n_ij d'une matrice 
# qui correspondent à des valeurs manquantes :
n_miss(dvf_Paris) 

# Pour les avoir en proportion du nombre total de cellules du tableau
# et non des seules lignes comme précédemment, 
# le résultat est déjà en pourcentage, sinon utiliser `prop_miss(RP)`)
pct_miss(dvf_Paris) 

# Ci-dessous : nombre de cellules du tableau ou de n_ij d'une matrice 
# qui correspondent à des valeurs renseignées :
n_complete(dvf_Paris) 
#en proportion
pct_complete(dvf_Paris) 
```

On peut ensuite visualiser le nombre de valeurs manquantes par variable, avec la fonction `gg_miss_var()` du même package. On peut également demander dans `gg_miss_var()` à ce que les valeurs soient en pourcentage, avec l'argument `show_pct=TRUE`.

```{r warning=FALSE}
# 1er type de visualisation des valeurs manquantes
dvf_Paris %>%  gg_miss_var()
```

On peut aussi réaliser des graphiques montrant le nombre de valeurs manquantes pour l'ensemble des variables numériques de la base, en fonction d'une autre variable (y compris de nature 'factor'), avec l'argument `fct=` dans `gg_miss_fct()`. Cela est intéressant pour voir si certaines valeurs manquantes des variables sont liées à des valeurs observées d'autres variables, qu'elles soient quantitatives ou qualitatives (et dans ce cas, est-ce que les valeurs manquantes se retrouvent davantage dans certaines modalités plus que d'autres ?). Par exemple, ici, selon le type de lots, ou la surface du logement :

```{r warning=FALSE}
# on filtre sur les variables numériques car on ne veut pas que la sortie nous affiche des variables comme le code postal.
dvf_Paris %>% select(where(is.numeric), nombre_lots) %>%
  gg_miss_fct(fct = nombre_lots)
dvf_Paris %>% select(where(is.numeric), -(code_postal)) %>%
  filter(surface_reelle_bati<250) %>%
  gg_miss_fct(fct = surface_reelle_bati)
```

On voit que les valeurs manquantes sont plus nombreuses pour les variables de numéro et surface carrez des lots 2, 3, 4 et 5 en proportion du nombre de lots, ou encore on observe qu'il y a un petit peu de valeurs manquantes pour les variables de prix au m2 ou de valeur foncière quant le nombre de lots est égal à 0. Elles ne se distribuent donc pas de manière uniforme selon la variable du nombre de lots, mais c'est assez logique ici et on comprend mieux les nombreuses valeurs manquantes pour les variables indiqués "lot2", "lot3", etc, car s'il n'y a qu'un ou deux lots c'est normal qu'il n'y ait pas d'informations sur ces variables. En revanche, on note que même avec 0 lot, on peut avoir l'information sur la surface, le prix/la valeur foncière, puisque ce n'est pas toujours manquant.

Plus généralement, la fonction `gg_miss_upset()` de ce même package **`naniar`** permet de visualiser des dépendances entre les valeurs manquantes des variables :

```{r}
dvf_Paris %>% select(where(is.numeric)) %>% gg_miss_upset()
```

Cela nous montre qu'il y a beaucoup d'observations où on a des valeurs manquantes pour 5 variables indiquées, qu'ensuite le cas le plus probable c'est des valeurs manquantes pour 4 de ces 5 variables, etc.

Enfin, il est possible d'appliquer la fonction `geom_miss_point()` à une fonction `ggplot`, dans ce cas les valeurs manquantes de la ou des variables sont remplacées par des valeurs 10% plus basses que la valeur minimum observée des variables, et cela afin de les visualiser.

Il existe bien sûr bien d'autres packages, comme **`funModeling`**, **`Amelia`** et sa fonction `missmap()`, ou encore **`visdat`** et sa fonction `vis_miss()`. Enfin, d'autres packages comme **`VIM`** ou **`MICE`** permettent, non seulement de visualiser ces valeurs manquantes, mais également de leur appliquer des techniques pour les "gérer", c'est ce que l'on va voir maintenant en résumé.

### Gérer les valeurs manquantes

Il est bien de connaître le nombre et la proportion de valeurs manquantes dans nos données, comment ces dernières se répartissent entre elles, etc., mais il faut aussi comprendre quel impact elles peuvent avoir sur des analyses statistiques, de régressions ou autres algorithmes.

Dans une base de données tirée d'une enquête, les valeurs manquantes peuvent provenir d'une non-réponse de la part de l'enquêté (que ce soit un individu ou une entreprise), cette non-réponse pouvant être "totale" (on a aucune donnée pour cet enquêté alors qu'il fait partie de l'échantillon) ou "partielle" (on a une partie des réponses mais pas à toutes les questions et donc des variables parfois avec des valeurs manquantes) ; ou bien encore elles peuvent être dues à une mauvaise saisie de l'information par l'enquêteur. La pondération, si elle est présente dans une enquête, peut permettre de corriger cette non-réponse totale, voire partielle.

Les conséquences des valeurs manquantes dans une base de données dépendent de plusieurs choses : on doit d'abord se demander si l'information perdue aurait été pertinente et/ou aurait apporté un élément particulier/supplémentaire. Ensuite, la perte éventuelle d'information est-elle importante, en nombre/en proportion. Et enfin (et surtout), peut-elle créer un biais lors de l'estimation et précision du phénomène que l'on souhaite observer, décrire, analyser, etc. Selon l'importance de ces conséquences, il faut traiter ces valeurs manquantes, c'est-à-dire utiliser une procédure la plus adaptée possible selon le potentiel biais repéré.

Traditionnellement dans la littérature, on distingue 3 types de valeurs manquantes :

-   valeur manquante entièrement due au hasard ('MCAR' pour Missing completely at random) : il n'y a pas de lien entre la valeur manquante pour une variable donnée et les autres variables, dit autrement la probabilité pour une variable qu'elle ait une valeur manquante est constante dans les données, elle ne diffère pas selon d'autres caractéristiques des individus ;
-   valeur manquante due au hasard ('MAR' pour Missing at random) : il y a un lien entre la valeur manquante pour une variable donnée et les valeurs observées d'autres variables, c'est-à-dire que la probabilité pour une variable qu'elle ait une valeur manquante dépend d'autres variables (de leurs valeurs observées), elle ne sera donc pas la même selon les individus, c'est ce qu'on essayait de regarder lorsqu'on a utilisé plus haut la fonction `gg_miss_fct(fct=)` ;
-   valeur ne manquant pas au hasard ('NMAR' pour Non missing at random) : il y a un lien entre la valeur manquante pour une variable et les valeurs manquantes/non observées d'autres variables. Ce sont celles qui risquent d'entraîner des biais importants si on ne les traite pas, c'est ce qu'on essayait de regarder plus haut également avec la fonction `gg_miss_upset()` cette fois.

Comment alors les gérer ? En pratique, il est d'usage lorsque la proportion de valeurs manquantes ne dépasse pas 5% des données de ne rien faire de particulier ou simplement de les supprimer (vous pouvez pour la savoir utiliser les premières fonctions du package **`naniar`** présentées précédemment). Sinon, on essaye d'appliquer plusieurs méthodes, simples ou plus complexes.

Dans le cas de valeurs manquantes entièrement dues au hasard (MCAR) et/ou d'une faible proportion des valeurs manquantes dans le total de la table de données, on peut décider de supprimer toutes les lignes qui contiennent au moins une valeur manquante, afin d'avoir une table de données complètes, on peut utiliser la fonction `na.omit()` ou `complete.cases()` ; attention à ne pas remplacer votre table de données initiale en réalisant cette procédure. On ne va pas s'essayer à le faire ici car on a vu au tout début de cette section que pour certaines variables cela concernait pratiquement toutes les observations (ici les ventes observées), la conséquence c'est qu'ici on va supprimer toutes les lignes car une ligne a forcément une valeur manquante dans une des variables. Le code serait celui-ci :

```{r eval=FALSE}
dvf_Paris_sansNA <- na.omit(dvf_Paris)
# OU : 
# dvf_Paris_sansNA <- dvf_Paris[complete.cases(dvf_Paris), ]
```

Des techniques d'imputation simple peuvent également être utilisées. On peut par exemple remplacer les valeurs manquantes d'une variable quantitative par sa moyenne ou sa médiane, pour cela on peut utiliser la fonction `replace_na()` du package **`tidyr`**, ou `impute()` du package **`Hmisc`**, ou encore `na.aggregate()` du package **`zoo`** On donne ainsi une valeur "artificielle" pour remplacer la valeur manquante. Dans le cas de variables qualitatives, on peut, de même, imputer la modalité dominante (avec la fonction `mode()` du package **`Hmisc`** ; ou avec l'argument `mode=` du package **`zoo`**). Par exemple, voici les codes pour remplacer les valeurs manquantes de la variable 'pxm2' par sa médiane (la base n'étant pas propre il vaut mieux utiliser la médiane que la moyenne) :

```{r eval=FALSE, include=TRUE}
dvf_Paris %>% mutate(px_m2_bis = replace_na(px_m2, median(px_m2, na.rm=TRUE))) %>% 
  select(px_m2, px_m2_bis) %>% filter(is.na(px_m2))

library(Hmisc)
dvf_Paris$px_m2_bis <- with(dvf_Paris, impute(px_m2, median))

library(zoo)
dvf_Paris$px_m2_bis <- na.aggregate(dvf_Paris$ px_m2, FUN = median)
```

On peut néanmoins réaliser ce type d'imputation simple de manière un petit peu plus subtile. Par exemple, si la moyenne de la variable diffère sensiblement selon une autre variable (catégorielle), dans ce cas, on va plutôt remplacer les valeurs manquantes de la variable selon la moyenne associée à chaque modalité de cette autre variable en ajoutant un `group_by()` avant la fonction `mutate()` si on utilise la fonction `replace_na()` comme dans l'exemple précédent.

Si on ne veut pas supprimer ces lignes d'observations et perdre ainsi d'autres informations (celles des variables pour lesquelles la valeur était renseignée pour cette même observation), on peut simplement créer une variable indicatrice de valeur manquante, remplacer les `NA` par '999' comme dans notre base de données actuelle pour des variables quantitatives, ou par une modalité 'Manquant' ou 'Missing' pour des variables qualitatives.

Plusieurs autres méthodes existent également dans le cas de valeurs manquantes dues au hasard (MAR), en voici la liste pour information et sans prétention d'exhaustivité : - analyse pondérée pour des valeurs MAR qui consiste à calculer la probabilité qu'une observation soit complète et ensuite à affecter à chacune des observations complètes, un poids inversement proportionnel à cette probabilité ; - imputation de la dernière observation pour des données temporelles ; - imputation "hot-deck" qui consiste à remplacer la valeur manquante par une valeur observée chez un autre individu ayant les mêmes caractéristiques, ou "cold-deck" (même démarche que précédement, sauf que la valeur imputée vient d'une autre source) ; - imputation par le "plus proche voisin" en utilisant une fonction de distance basée sur plusieurs autres variables/caractéristiques de l'individu ; - imputation par un modèle de régression où l'on va remplacer la valeur manquante par une valeur prédite obtenue par régression sur données complètes de la variable comportant des valeurs manquantes.

Il y a aussi des techniques plus complextes d'imputation multiple qui consiste à créer plusieurs valeurs possibles pour une valeur manquante d'une variable, cela peut être adaptée là aussi lorsque les valeurs manquantes sont dues au hasard (MAR).

Vous trouverez de multiples ressources sur internet dans des ouvrages libres d'accès, ou vous pouvez aller voir un des chapitres de l'ouvrage principal support du cours (Husson, 2018), avec des exemples d'utilisation.

### Détecter et "visualiser" les valeurs aberrantes

On va continuer avec cette base de données en se concentrant sur les variables de valeur foncière, de surface, de nombre de pièces, etde prix au m2, en s'intéressant maintenant aux valeurs aberrantes.

On peut d'abord étudier la distribution de ces variables : la fonction `get_summary_stats()` du package `rstatix` permet de donner les statistiques de distribution des variables, on propose d'afficher les principales ici :

```{r message=FALSE, warning=FALSE}
library(rstatix)
dvf_Paris %>%
  get_summary_stats(valeur_fonciere, nombre_pieces_principales, 
                    surface_reelle_bati, px_m2, nombre_lots,
                    show=c("n","mean", "median", "min", "max","q1", "q3")) %>% gt()
```

Cela nous permet de comprendre qu'il y a probablement encore quelques filtres à effectuer pour avoir une base propre et cohérente, non seulement sur les valeurs aberrantes - on va y venir - mais aussi sur les valeurs minimum. Peut-on ainsi vendre un appartement avec 0 pièce principale ? Ou d'une surface d'1 m2 ? Ou pour 0€ ? On va donc filtrer la base sur ces éléments. De plus, comme on a vu qu'il y avait beaucoup de valeurs manquantes sur les variables reliées aux numéros de lot supérieur à 3, on va retenir dans la base les seules ventes avec des lots compris entre 1 et 2 (en gros, par exemple, un appartement - lot 1 - ou un appartement et une cave - lot 2).

```{r}
# Filtre pour réduire la base et la rendre plus propre, et sélection en supprimant les variables qui ne nous intéressents plus
dvf_Paris <- dvf_Paris %>% 
  filter(nombre_lots %in% c(1,2) & nombre_pieces_principales>0 &
           surface_reelle_bati>=9 & valeur_fonciere>0) %>% 
  select(-c(lot5_surface_carrez, lot5_numero, lot4_surface_carrez,
            lot4_numero, lot3_surface_carrez, lot3_numero))
```

On peut relancer nos statistiques précédentes, pour vérifier que c'est plus cohérent et aussi pour étudier maintenant les maximum de certaines variables, c'est en effet une première manière de voir de potentielles valeurs aberrantes.

```{r}
dvf_Paris %>%
  get_summary_stats(valeur_fonciere, nombre_pieces_principales, 
                    surface_reelle_bati, px_m2, 
                    show=c("n","mean", "median", "min", "max","q1", "q3","iqr")) %>% 
  gt() %>% 
  fmt_number(columns = 3:8, sep_mark = " ", decimals = 1)

```

On peut également faire quelques graphs sur cette pour mieux visualiser ces valeurs aberrantes, un histogramme, ou une "boîte à moustache" (seule ou en relation avec une autre variable) :

```{r}
dvf_Paris %>%  ggplot() + aes(x=px_m2) + geom_histogram(bins=50)

dvf_Paris %>%  ggplot() + aes(x = annee, y = px_m2) +  geom_boxplot() +
  coord_flip()
```

On voit bien que ce soit avec l'histogramme ou la boîte à moustâche des points aberrants qui "écrasent" les représentations graphiques, de telle sorte qu'on ne voit même pas la distribution, en particulier dans le Boxplot la "boîte" en elle-même.

Pour rappel, dans un boxplot, par défaut un point est affiché comme aberrant s'il est en dehors de l'intervalle suivant : $I=[Q_{1}−1.5×IQR ; Q_{3}+1.5×IQR]$, `IQR` étant l'intervalle interquartile donc la différence entre `Q1` et `Q3`.

Mais s'agit-il de "vraies" valeurs aberrantes ? Combien d'observations concernent-elles ? La fonction `boxplot.stats()` permet de récupérer les valeurs des observations indiquées comme aberrantes, comme cela on peut créer ensuite une variable indiquant si oui ou non l'observation a une valeur "aberrante". Faisons-cela pour la variable de valeur foncière.

```{r warning=FALSE, message=FALSE}
# On récupère les valeurs de la partie 'out' des sorties de la fonction
# 'boxplot.stats', qui correspondent aux valeurs de tout point de données
# qui se situe au-delà des extrêmes de la boxplot
val_outliers <- boxplot.stats(dvf_Paris$px_m2)$out 

# On crée une variable dans notre table d'"identification" de ces outliers
# avec comme modalité "vraie" si l'observation a une valeur "outliers", 
# sinon "Faux"
dvf_Paris <- dvf_Paris %>% 
  mutate(px_m2_outliers = 
           case_when(px_m2 %in% c(val_outliers) ~ "Vrai",
                     TRUE ~ "Faux"))

# Puis on regarde la répartition avec la fonction `tabyl()` du package `janitor()` 
library(janitor)
dvf_Paris %>% tabyl(px_m2_outliers) %>% 
  adorn_pct_formatting() %>% adorn_totals("row") %>% gt()
```

On y lit que pour cette variable, il y aurait près de 13,2% de valeurs aberrantes telles qu'indiquées par le boxplot, ce qui correspondant à 19 644 observations, c'est beaucoup ! On peut regarder plus précisément à quelles observations elles correspondent, en sélectionnant avec la variable créée et en triant par ordre croissant ou décroissant.

```{r}
dvf_Paris %>% filter(px_m2_outliers=='Vrai') %>% 
  select(px_m2, nombre_pieces_principales, 
         surface_reelle_bati, valeur_fonciere)  %>% 
  arrange(px_m2) %>% head(5) %>% 
  gt()

dvf_Paris %>% filter(px_m2_outliers=='Vrai') %>% 
  select(px_m2, nombre_pieces_principales, 
         surface_reelle_bati, valeur_fonciere)  %>% 
  arrange(desc(px_m2)) %>% head(5) %>% 
  gt()
```

On voit donc qu'il y a des valeurs considérées comme aberrantes en bas de la distribution (valeurs trop faible) et en haut de la distribution ; ici sur les 5 premières valeurs, on a bien l'impression que ce sont des valeurs aberrantes. Pour être plus précis, on peut calculer les valeurs seuils bas et haut puisqu'on connaît la formule. Le seuil bas sera : `r quantile(dvf_Paris$px_m2, 0.25)-1.5*(IQR(dvf_Paris$px_m2))` et le seuil haut : `r quantile(dvf_Paris$px_m2, 0.75)+1.5*(IQR(dvf_Paris$px_m2))`. Ces seuils peuvent donc être vraisemblables en réalité. Il est donc important de comprendre ces valeurs aberrantes, cela peut parfois correspondre à des observations intéressantes à conserver, il ne s'agit pas juste de les identifier pour les exclure directement ensuite des analyses.

Il existe d'autres méthodes (méthode basée sur les percentiles ; méthode de Hampel), et d'autres tests : par exemple, le package **`outliers`** vous permet de tester si une valeur (max ou min) est bien une valeur aberrante avec la fonction `grubbs.test()` (attention bis : à utiliser avec grande précaution et beaucoup de parcimonie), ou avec le package **`EnvStats`** et la fonction `rosnerTest()` pour détecter plusieurs "outliers" à la fois.

Pour gérer ces variables aberrantes, on peut les supprimer bien sûr si l'on est sûr que la valeur de la variable n'est pas "normale", par exemple si on a une variable de salaire avec des modalités inférieures à 0, oui dans ce cas ce sont des mauvais outliers (et d'ailleurs peut-être même pas identifiés comme tel statistiquement) et on peut les supprimer ; de même pour des variables de résultats économiques, on va souvent élaguer la distribution en retirant les 1% (par exemple) du bas et du haut de la distribution pour supprimer des potentiels outliers. On peut tenter cette méthode ici en filtrant les données avant de calculer la distribution de la variable.\
Sinon, on les isole en créant une variable dichotomique "0/1" ou "Faux/Vrai" ; ou on crée une variable qualitative avec plusieurs catégories (cf. sous-section suivante).

Dans les graphiques, en particulier les boîtes à moustache, on peut les supprimer visuellement avec l'option `outlier.shape = NA` et mettre ensuite une échelle plus réduite (avec `ylim=c( , )`) pour que le graphique soit plus lisible, mais il faut alors bien préciser dans la légende que certaines valeurs ne sont pas visibles sur le graphique car retirées ; attention à ne pas les supprimer de la base sur laquelle est réalisée la boxplot car sinon cela va modifier les indicateurs (en particulier de moyenne mais pas seulement). Dans un histogramme, on peut de même jouer sur l'échelle.

```{r}
dvf_Paris %>%  ggplot() + aes(y = px_m2) + 
  geom_boxplot(outlier.shape = NA) + coord_flip(ylim = c(quantile(dvf_Paris$px_m2, 0.01),quantile(dvf_Paris$px_m2, 0.99))) +
  labs(title = "Distribution des valeurs foncières en euros", y="", x="", 
       caption="Rq : les valeurs en-dessous du 1% de la distribution et celles au-dessus
       du 99% de la distribution ne sont pas \naffichées sur le graphique.") +
  theme(plot.caption = element_text(hjust=0))

dvf_Paris %>% ggplot() + aes(px_m2) + 
  geom_histogram(bins=10000) +  coord_cartesian(xlim=c(quantile(dvf_Paris$px_m2, 0.01),
                                                       quantile(dvf_Paris$px_m2, 0.99))) +
  labs(title = "Distribution des valeurs foncières en euros", y="", x="Prix au m2",
       caption="Rq : les valeurs en-dessous du 1% de la distribution et celles au-dessus 
       du 99% de la distribution ne sont pas \naffichées sur le graphique.") +
  theme(plot.caption = element_text(hjust=0))
```

C'est un peu mieux mais on voit que la boxplot est toujours écrasée... et sur l'histogramme on voit qu'il y a probablement encore un problème pour les valeurs faibles du prix au m2 qu'il faudrait "nettoyer". On peut refaire le graphique en élaguant davantage en bas de la distribution par exemple :
```{r}
dvf_Paris %>% ggplot() + aes(px_m2) + 
  geom_histogram(bins=20000) +  coord_cartesian(xlim=c(quantile(dvf_Paris$px_m2, 0.05),
                                                       quantile(dvf_Paris$px_m2, 0.95))) +
  labs(title = "Distribution des valeurs foncières en euros", y="", x="Prix au m2",
       caption="Rq : les valeurs en-dessous du 5% de la distribution et celles au-dessus du 95% de la distribution ne sont pas affichées \nsur le graphique.") +
  theme(plot.caption = element_text(hjust=0))
```


### Découper en classes une variable quantitative

On peut enfin découper en classes une variable quantitative et en faire donc une variable qualitative. On utilise pour cela la fonction `cut()` du langage de base de `R`. On peut par exemple découper la variable selon les principaux indicateurs de la distribution.

```{r message=FALSE}
#dvf_Paris %>% get_summary_stats(px_m2)
dvf_Paris$px_m2_cat <- cut(dvf_Paris$px_m2,  
                           breaks = c(0,
                                      quantile(dvf_Paris$px_m2,0.25), 
                                      mean(dvf_Paris$px_m2),
                                      max(dvf_Paris$px_m2)),
                           labels=c("entre 0 et le Q1(9038€)",
                                    "Entre le Q1 et la moyenne (17121€)",
                                    "Entre la moyenne et le maximum"))
dvf_Paris %>% tabyl(px_m2_cat) %>% adorn_pct_formatting() %>% 
  adorn_totals("row") %>% gt()
```

On a une classe majoritaire (du Q1 à la moyenne), mais cela nous permet de distinguer 2 classes pour lesquelles le montant du prix au m2 est soit plutôt faible, soit plutôt élevé.

À noter que si la variable quantitative en question a des valeurs manquantes, il faudra utiliser la fonction `fancycut()` ou `wafflecut()` du package **`fancycut`**, l'inconvénient est que cela nous oblige à indiquer les valeurs des différents indicateurs de la distribution.

<!-- Pour trouver le même résultat que précédemment, le code serait alors le suivant : -->

```{r eval=FALSE, include=FALSE}
library(fancycut)
dvf_Paris$px_m2_cat1 <- fancycut(dvf_Paris$px_m2_cat, 
                                 '1'='(0,9000]', 
                                 '2'='(9000,17000]', 
                                 '3'='(17000.10606667]', 
                                 na.bucket='Manquant')
dvf_Paris %>% tabyl(px_m2_cat1) %>% adorn_pct_formatting() %>% 
  adorn_totals("row") %>% gt()
```
